# -*- coding: utf-8 -*-
"""Credit_Risk.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1viNLqCz8mNmub12ZNySp0RN33UONim3C
"""

import pandas as pd
import zipfile
import numpy as np
from google.colab import drive
import chardet
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix, roc_curve, auc

!pip install dask[dataframe]

!pip install --upgrade xgboost scikit-learn --no-cache-dir

zip_path = "/content/default+of+credit+card+clients.zip"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall("/content/")

file_path = "/content/default of credit card clients.xls"
df = pd.read_excel(file_path, header=1)

df.head()

df.info()

"""Rename the target column for consistency"""

df.rename(columns={"default payment next month": "DEFAULT"}, inplace=True)

"""Drop the ID column if it exists


"""

df.drop(columns=["ID"], errors='ignore', inplace=True)

"""Convert all columns to numeric"""

df = df.apply(pd.to_numeric, errors='coerce')

df.info()

"""Dataset is now successfully loaded and pre-processed.

    

*   All columns are numeric
*   No missing values detected

Exploratory Data Analysis (EDA)

1)class distribution
"""

plt.figure(figsize=(6,4))
sns.countplot(x=df["DEFAULT"])
plt.title("Distribution of Default (Target Variable)")
plt.xlabel("DEFAULT (0 = No Default, 1 = Default)")
plt.ylabel("Count")
plt.show()

"""From the above chart:

Class Imbalance:

Majority of the records belong to class 0 (No Default).

Fewer records belong to class 1 (Default).

Need to handle this class imbalance to improve model performance.

Summary Statistics:
"""

df.describe()

"""From the above plot:
1Ô∏è‚É£ Check for Unusual or Incorrect Values
EDUCATION (0 to 6)

Expected values: 1 = Graduate, 2 = University, 3 = High School, 4 = Others
But 0, 5, and 6 appear, which may need to be grouped as "Others."
MARRIAGE (0 to 3)

Expected values: 1 = Married, 2 = Single, 3 = Others
0 is unexpected and might need correction.
PAY_0 to PAY_6 (Repayment Status)

Expected: Values from -2 (no delay) to 8 (worst delay).
The values seem correct.


2Ô∏è‚É£ Check for Outliers
LIMIT_BAL (Credit Limit)

Min = 10,000, Max = 1,000,000
Large range ‚Üí Need scaling (StandardScaler or MinMaxScaler)
BILL_AMT1 to BILL_AMT6 (Bill Amount)

Some values negative (e.g., -170,000) ‚Üí Check if errors or refunds.
PAY_AMT1 to PAY_AMT6 (Payment Amount)

Max = 1.6 million, very high values ‚Üí Likely outliers.

**Data Preprocessing**

Fixing Categorical Values
"""

df["EDUCATION"] = df["EDUCATION"].replace({0: 4, 5: 4, 6: 4})
df["MARRIAGE"] = df["MARRIAGE"].replace(0, 3)

"""Scale Numerical Features"""

scale_cols = ["LIMIT_BAL", "AGE", "BILL_AMT1", "BILL_AMT2", "BILL_AMT3",
              "BILL_AMT4", "BILL_AMT5", "BILL_AMT6", "PAY_AMT1", "PAY_AMT2",
              "PAY_AMT3", "PAY_AMT4", "PAY_AMT5", "PAY_AMT6"]

scaler = StandardScaler()
df[scale_cols] = scaler.fit_transform(df[scale_cols])

"""Handle Class Imbalance Using SMOTE"""

X = df.drop(columns=["DEFAULT"])
y = df["DEFAULT"]

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

print(y_resampled.value_counts())

"""Now, the distribution is equal distribution for DEFAULT (0 and 1), meaning the dataset is balanced and ready for training.

**Split the Data into Train/Test Sets**
"""

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)

print("Training Data Shape:", X_train.shape)
print("Testing Data Shape:", X_test.shape)

"""**Train and Evaluate Machine Learning Models**

Logistic Regression
"""

param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10],
    'solver': ['liblinear', 'lbfgs']
}

lr_grid = GridSearchCV(LogisticRegression(class_weight="balanced", random_state=42),
                       param_grid, cv=5, scoring='roc_auc', n_jobs=-1)

lr_grid.fit(X_resampled, y_resampled)

print("üîç Best Logistic Regression Parameters:", lr_grid.best_params_)
print("üìä Best AUC-ROC Score from GridSearch:", lr_grid.best_score_)

"""Train the Optimized Logistic Regression Model"""

best_lr_model = LogisticRegression(
    C=0.1,
    solver='lbfgs',
    class_weight="balanced",
    random_state=42
)

best_lr_model.fit(X_resampled, y_resampled)

y_pred = best_lr_model.predict(X_test)

print("Logistic Regression Performance on Real-World Data:")
print(classification_report(y_test, y_pred))

y_probs = best_lr_model.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, y_probs)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, color='blue', lw=2, label='Logistic Regression (AUC = {:.3f})'.format(roc_auc))
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line for reference
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate (FPR)")
plt.ylabel("True Positive Rate (TPR)")
plt.title("ROC Curve - Logistic Regression")
plt.legend(loc="lower right")
plt.show()
print(" AUC-ROC Score:", roc_auc)

plt.figure(figsize=(6,4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

"""AUC-ROC Score = 0.745, which means:

The model is better than random guessing (0.5) but not highly accurate.

Summary:



1.   Precision- The percentage of correct predictions for each class
2.   Recall- The model's ability to detect actual defaults
3.   F1-score- Balance between precision and recall
4.   Accuracy- **69% overall accuracy**

Balanced Precision & Recall: Since recall is high for both classes, the model is not biased towards predicting only non-defaulters.

LightGBM
"""

param_grid = {
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7, 9],
    'num_leaves': [20, 30, 40, 50],
    'min_child_samples': [10, 20, 30, 50]
}

lgb_grid = RandomizedSearchCV(
    LGBMClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='roc_auc',
    n_iter=10,
    n_jobs=-1,
    random_state=42
)

lgb_grid.fit(X_resampled, y_resampled)

print("üîç Best LightGBM Parameters:", lgb_grid.best_params_)
print("üìä Best AUC-ROC Score from GridSearch:", lgb_grid.best_score_)

"""Train LightGBM with Best Hyperparameters"""

best_lgb_model = LGBMClassifier(
    num_leaves=40,
    n_estimators=200,
    min_child_samples=20,
    max_depth=7,
    learning_rate=0.2,
    random_state=42
)

best_lgb_model.fit(X_resampled, y_resampled)

"""Evaluate LightGBM on Real Test Data"""

y_pred_lgb = best_lgb_model.predict(X_test)
print(" LightGBM Performance on Real-World Data:")
print(classification_report(y_test, y_pred_lgb))

plt.figure(figsize=(6,4))
sns.heatmap(confusion_matrix(y_test, y_pred_lgb), annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - LightGBM")
plt.show()

"""ROC-AUC Curve for LightGBM"""

y_probs_lgb = best_lgb_model.predict_proba(X_test)[:,1]
fpr_lgb, tpr_lgb, _ = roc_curve(y_test, y_probs_lgb)
roc_auc_lgb = auc(fpr_lgb, tpr_lgb)
plt.figure(figsize=(8,6))
plt.plot(fpr_lgb, tpr_lgb, color='green', lw=2, label='LightGBM (AUC = {:.3f})'.format(roc_auc_lgb))
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate (FPR)")
plt.ylabel("True Positive Rate (TPR)")
plt.title("ROC Curve - LightGBM")
plt.legend(loc="lower right")
plt.show()
print(" LightGBM AUC-ROC Score:", roc_auc_lgb)

"""Summary:

LightGBM model is performing exceptionally well, with:

AUC-ROC = 0.982  ‚Üí Almost perfect model discrimination between defaulters and non-defaulters.
Accuracy = 93%  ‚Üí Very high prediction accuracy.
Precision & Recall = 0.93  ‚Üí Well-balanced model, making both correct positive and negative predictions.

1Ô∏è‚É£ Highly Effective Model

AUC-ROC of 0.982 suggests a near-perfect model.
Precision & Recall both ~93%, meaning minimal false predictions.

2Ô∏è‚É£ Better Than Logistic Regression

Logistic Regression had an AUC of ~0.74, while LightGBM significantly improved to 0.98.
This shows LightGBM captures non-linear relationships better.

3Ô∏è‚É£ Low False Negatives (High Recall for Class 1)

Recall = 0.89 for Defaulters (1) ‚Üí The model correctly identifies most risky customers.
This is crucial for credit risk prediction to avoid lending to high-risk clients.

RandomForestClassifier
"""

rf_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf_grid = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    rf_param_grid,
    cv=5,
    scoring='roc_auc',
    n_iter=10,
    n_jobs=-1,
    random_state=42
)

rf_grid.fit(X_resampled, y_resampled)

print("üîç Best Random Forest Parameters:", rf_grid.best_params_)

best_rf_model = RandomForestClassifier(
    n_estimators=200,
    max_depth=15,
    min_samples_split=10,
    min_samples_leaf=2,
    random_state=42
)

best_rf_model.fit(X_resampled, y_resampled)

y_pred_rf = best_rf_model.predict(X_test)

print(" Random Forest Performance on Real-World Data:")
print(classification_report(y_test, y_pred_rf))

plt.figure(figsize=(6,4))
sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Random Forest")
plt.show()

"""ROC-AUC Curve for Random Forest"""

y_probs_rf = best_rf_model.predict_proba(X_test)[:,1]

fpr_rf, tpr_rf, _ = roc_curve(y_test, y_probs_rf)
roc_auc_rf = auc(fpr_rf, tpr_rf)

plt.figure(figsize=(8,6))
plt.plot(fpr_rf, tpr_rf, color='blue', lw=2, label='Random Forest (AUC = {:.3f})'.format(roc_auc_rf))
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate (FPR)")
plt.ylabel("True Positive Rate (TPR)")
plt.title("ROC Curve - Random Forest")
plt.legend(loc="lower right")
plt.show()

print(" Random Forest AUC-ROC Score:", roc_auc_rf)

"""Summary:

Key Observations

‚úÖ High AUC-ROC (0.956) ‚Üí Strong model for distinguishing between defaulters and non-defaulters.

‚úÖ Balanced Precision & Recall ‚Üí Good at both identifying and avoiding misclassification of defaulters.

‚úÖ Accuracy = 87% ‚Üí Reliable, but slightly lower than LightGBM.

‚ö† Recall (0.84) is lower than LightGBM (0.89) ‚Üí Might miss some defaulters.

Performance Comparison Bar Plot
"""

models = ["Logistic Regression", "Random Forest", "LightGBM"]
auc_roc_scores = [0.745, 0.956, 0.982]
accuracy_scores = [0.69, 0.87, 0.93]
precision_scores = [0.69, 0.89, 0.97]
recall_scores = [0.67, 0.84, 0.89]
f1_scores = [0.68, 0.87, 0.93]
bar_width = 0.15
index = np.arange(len(models))
plt.figure(figsize=(10, 6))
plt.bar(index, auc_roc_scores, bar_width, label="AUC-ROC", color='blue')
plt.bar(index + bar_width, accuracy_scores, bar_width, label="Accuracy", color='green')
plt.bar(index + 2*bar_width, precision_scores, bar_width, label="Precision", color='orange')
plt.bar(index + 3*bar_width, recall_scores, bar_width, label="Recall", color='red')
plt.bar(index + 4*bar_width, f1_scores, bar_width, label="F1-Score", color='purple')

plt.xlabel("Models")
plt.ylabel("Score")
plt.title("Model Performance Comparison")
plt.xticks(index + 2*bar_width, models)
plt.legend()
plt.show()

"""The bar chart compares overall performance metrics (AUC-ROC, Accuracy, Precision, Recall, F1-Score).

Key Insights:

1Ô∏è‚É£ LightGBM is the Best Model üöÄ

Highest AUC-ROC (0.982) ‚Üí Best at distinguishing defaulters vs non-defaulters.
Best Accuracy (93%) ‚Üí Most overall correct predictions.
Best Precision (0.97) ‚Üí Least false positives.
Best Recall (0.89) ‚Üí Detects defaulters better than other models.

2Ô∏è‚É£ Random Forest is Strong but Slightly Weaker than LightGBM üå≤

AUC-ROC (0.956) is high, but not as good as LightGBM.
Accuracy (87%) is lower than LightGBM.
Balanced Precision (0.89) & Recall (0.84) but lower than LightGBM.

3Ô∏è‚É£ Logistic Regression is the Weakest ‚ö†

AUC-ROC (0.745) is significantly lower.
Accuracy (69%) is much lower than both Random Forest & LightGBM.
Precision & Recall are the lowest, making it unsuitable for predicting defaults.

**LightGBM** is the best model for credit risk prediction based on all metrics.

Compare ROC-AUC Curves of All Models
"""

y_probs_lr = best_lr_model.predict_proba(X_test)[:,1]
y_probs_rf = best_rf_model.predict_proba(X_test)[:,1]
y_probs_lgb = best_lgb_model.predict_proba(X_test)[:,1]

fpr_lr, tpr_lr, _ = roc_curve(y_test, y_probs_lr)
roc_auc_lr = auc(fpr_lr, tpr_lr)

fpr_rf, tpr_rf, _ = roc_curve(y_test, y_probs_rf)
roc_auc_rf = auc(fpr_rf, tpr_rf)

fpr_lgb, tpr_lgb, _ = roc_curve(y_test, y_probs_lgb)
roc_auc_lgb = auc(fpr_lgb, tpr_lgb)

plt.figure(figsize=(8, 6))
plt.plot(fpr_lr, tpr_lr, color='blue', lw=2, label='Logistic Regression (AUC = {:.3f})'.format(roc_auc_lr))
plt.plot(fpr_rf, tpr_rf, color='red', lw=2, label='Random Forest (AUC = {:.3f})'.format(roc_auc_rf))
plt.plot(fpr_lgb, tpr_lgb, color='green', lw=2, label='LightGBM (AUC = {:.3f})'.format(roc_auc_lgb))
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')

plt.xlabel("False Positive Rate (FPR)")
plt.ylabel("True Positive Rate (TPR)")
plt.title("ROC Curve Comparison")
plt.legend(loc="lower right")

plt.show()

"""The ROC curve plot shows how well each model separates defaulters vs. non-defaulters.

Deploying the Credit Risk Prediction Model using Streamlit
"""

import numpy as np
import joblib

joblib.dump(best_lgb_model, "credit_risk_model.pkl")
print("‚úÖ Model saved successfully as 'credit_risk_model.pkl'!")

import joblib

# Load the saved model
model = joblib.load("credit_risk_model.pkl")

# Verify model prediction
sample_data = X_test.iloc[0].values.reshape(1, -1)  # Take a single test example
prediction = model.predict(sample_data)
print("‚úÖ Sample Prediction:", "High Risk" if prediction[0] == 1 else "Low Risk")

import streamlit as st
import joblib
import numpy as np

# Load the trained LightGBM model
model = joblib.load("credit_risk_model.pkl")

# üé® App Layout
st.set_page_config(page_title="Credit Risk Prediction", layout="centered")

st.markdown("<h1 style='text-align: center; color: #4CAF50;'>üí≥ Credit Risk Prediction üöÄ</h1>", unsafe_allow_html=True)
st.write("Fill in the details below to check the credit risk status.")

# üìå User Input Fields
LIMIT_BAL = st.number_input("üí∞ Credit Limit ($)", min_value=10000, max_value=1000000, step=5000)
AGE = st.number_input("üéÇ Age", min_value=18, max_value=80, step=1)
EDUCATION = st.selectbox("üéì Education Level", ["Graduate School", "University", "High School", "Others"])
MARRIAGE = st.selectbox("üíç Marital Status", ["Married", "Single", "Others"])
PAY_1 = st.slider("üìÜ Last Month's Repayment Status", -2, 8, 0)
BILL_AMT1 = st.number_input("üßæ Latest Bill Amount ($)", min_value=0, max_value=100000, step=500)
PAY_AMT1 = st.number_input("üí≥ Last Payment Amount ($)", min_value=0, max_value=100000, step=500)

# üîÑ Convert User Input to Model Format
education_map = {"Graduate School": 1, "University": 2, "High School": 3, "Others": 4}
marriage_map = {"Married": 1, "Single": 2, "Others": 3}

input_data = np.array([
    LIMIT_BAL, AGE, education_map[EDUCATION], marriage_map[MARRIAGE], PAY_1, BILL_AMT1, PAY_AMT1
]).reshape(1, -1)

# üîÆ Prediction Button
if st.button("üîç Predict Credit Risk"):
    prediction = model.predict(input_data)[0]
    if prediction == 1:
        st.error("‚ö†Ô∏è High Risk: This applicant is likely to default.")
    else:
        st.success("‚úÖ Low Risk: This applicant is unlikely to default.")

st.write("üöÄ Built with LightGBM & Streamlit")